#!/bin/bash
#SBATCH --job-name=admixpart        
#SBATCH --ntasks=1 
#SBATCH --cpus-per-task=4
#SBATCH --mem=80GB 
#SBATCH --time=72:00:00
#SBATCH --account=pathogens
#SBATCH --export=none
#SBATCH --output=%x.%j.out
#SBATCH --error=%x.%j.out
#SBATCH --mail-user=alexander.piper@agriculture.vic.gov.au
#SBATCH --mail-type=ALL

#--------------------------------------------------------------------------------
#-                                  HEADER		                                -
#--------------------------------------------------------------------------------

# Welcome to the insect SkimSeq Pipeline
# Developed by Alexander Piper 
# Contact: alexander.piper@agriculture.vic.gov.au

# This script applies NGSadmix to Genotype likelihood files generated by ANGSD

if [ -z "$SLURM_ARRAY_TASK_COUNT" ]; then 
  echo SLURM_ARRAY_TASK_COUNT unset; 
  echo You must launch this job as an array
  echo see https://slurm.schedmd.com/job_array.html
  echo for info on how to run arrays
  exit 1
fi

Index=admixpart_job_index.txt

# Check sequence index file exists
if [[ ! -f "${Index}" ]]; then
  echo "Error sequence index file ${Index} does not exist"
  exit 1
fi

#--------------------------------------------------------------------------------
#-                                  Parse Inputs                                -
#--------------------------------------------------------------------------------
# Default to empty inputs
beagle=""

# Function: Print a help message.
usage() {                                 
  echo "Usage: $0 [ -R Reference Genome ] [ -O output directory ] " 1>&2 
}
# Function: Exit with error.
exit_abnormal() {                         
  usage
  exit 1
}

# Get input options
OPTIND=1
while getopts ":B:S:O:t" options; do       
  # use silent error checking;
  case "${options}" in
    B)                             
      beagle=${OPTARG}
	  # Test if exists
	  if [ ! -f "$beagle" ] ; then  
        echo "Error: -B ${beagle} doesnt exist"
        exit_abnormal
        exit 1
      fi
	  echo beagle=${beagle}	  
      ;;
    S)
	  sitelist=${OPTARG}
	  # Test if not empty
	  if [[ $sitelist ]]; then
		  # test if exists
		  if [ ! -f "$sitelist" ] ; then  
			echo "Error: -S ${sitelist} doesnt exist"
			exit_abnormal
			exit 1
		  fi
		echo sitelist=${sitelist}
	  fi
      ;;
    O)
      outdir=${OPTARG}
      echo outdir=${outdir}
      ;;
    t)
	  echo "-t has been set, subsampling all fastqs for testing purposes" >&2
	  test_run='true'
    ;;
	:) 
	# Exit If expected argument omitted
      echo "Error: -${OPTARG} requires an argument."
      exit_abnormal 
	  ;;
    *) 
	# Exit If unknown (any other) option
      exit_abnormal
      ;;
  esac
done
shift $((OPTIND -1))

#--------------------------------------------------------------------------------
#-                                    Preparation                               -
#--------------------------------------------------------------------------------
# Read input params
bamlist=$(sed -n ${SLURM_ARRAY_TASK_ID}p ${Index})

# Get sample name from beagle file
Sample=$(basename ${beagle} .beagle.gz)
echo Sample=${Sample}

#set outdir file name
outname=$(echo $Sample $(echo $(basename ${bamlist}) | sed 's/bamlist_//g' | sed 's/manifest_//g'| sed 's/.txt//g') $(basename ${sitelist} | sed 's/.sites.*$//g' ) | sed 's/ /-/g' )

# Make directories for outdirs
mkdir -p ${outdir}/${outname}

# Goto tmp
cd $TMPDIR
tmp_dir=$(mktemp -d -t ci-XXXXXXXXXX)
cd ${tmp_dir}
pwd

#--------------------------------------------------------------------------------
#-                                  TEST	                                    -
#--------------------------------------------------------------------------------
# if -t is set, all fastqs will be subsampled
if [[ "$test_run" = true ]]; then
    # For quick testing - subsample input to just first 10000 sitees
    echo subsampling to first 10000 sites only
    #chr1=$(pigz -p ${SLURM_CPUS_PER_TASK} -cd ${beagle} | head -n 2 | tail -n 1 | awk -F '\t' '{print $1}' | awk -F '_' '{print $1}')
    pigz -p ${SLURM_CPUS_PER_TASK} -cd ${beagle} | head -n 1  > ${Sample}.test.beagle
    pigz -p ${SLURM_CPUS_PER_TASK} -cd ${beagle} | head -10000 >> ${Sample}.test.beagle
    mv ${Sample}.test.beagle ${Sample}.beagle
else 
    pigz -p ${SLURM_CPUS_PER_TASK} -cd ${beagle} > ${Sample}.beagle
fi

#--------------------------------------------------------------------------------
#-                           		Prune SNPs                                  -
#--------------------------------------------------------------------------------

if [ -n "$sitelist" ]; then
	echo "Subsetting to only those sites within sitelist: ${sitelist}"
    pigz -p ${SLURM_CPUS_PER_TASK} -cd ${sitelist} | tr ":" "_" > sites_to_keep
	cat ${Sample}.beagle | head -n 1  > ${Sample}.subset.beagle
	cat ${Sample}.beagle | grep -Fwf sites_to_keep >> ${Sample}.subset.beagle
	mv ${Sample}.subset.beagle ${Sample}.beagle
else 
	echo "no sites to remove"
fi

#--------------------------------------------------------------------------------
#-                            Run NGSadmix until convergence                    -
#--------------------------------------------------------------------------------
#Load Modules
module purge
module load HTSlib/1.15-GCC-11.2.0
module load evalAdmix/20221126-GCCcore-11.2.0 
module load Python/3.9.6-GCCcore-11.2.0
module load R/4.2.0-foss-2021b

# Original dataset file
cat ${bamlist} | sed 's/.bam//g' >> ${outname}_.txt
DATA_FILE="${outname}_.txt"

# Initialize partition counter
partition_counter=0
max_partitions=5

# Initial partition
current_files=("$DATA_FILE")

# Loop until no data points remain
while [[ ${#current_files[@]} -gt 0 && $partition_counter -lt $max_partitions ]]; do
  next_files=()

  for input_file in "${current_files[@]}"; do
    # Count total lines in the input file
    total_lines=$(wc -l < "$input_file")
    input_name=$(echo $input_file | sed 's/\..*$//g')

    # If only 1 sample is remaining, skip to the next file
    if [[ $total_lines -eq 1 ]]; then
      continue
    fi

    # Subset beagle to only the samples to be analysed within this partition
    cat ${Sample}.beagle | head -1 | tr "\t" "\n" > header

    # Get file to search
    printf  'marker\nallele1\nallele2\n' > search.txt
    cat ${input_file} | sed 's/.bam//g' >> search.txt
     
    # Get the rows that arent in the bamlist
    grep -vf search.txt header | sort | uniq > to_remove

    #Check if any samples need removing
    if grep -q '[^[:space:]]' to_remove; then
        echo "Subsetting beagle file to match bamlist"	
        # Get the index of the rows to remove and put in a variable
        index_rem=$(grep -nFf to_remove header | cut -d : -f 1 | tr "\n" ","| sed '$s/,$//')

        # Remove those that arent in bamlist cut complement removes them
        cat ${Sample}.beagle | cut --complement -f${index_rem} > ${Sample}.subset.beagle
        echo $(expr $( cat ${Sample}.subset.beagle | head -1 | tr "\t" "\n" | uniq | wc -l) - 3) samples remaining after subsetting
    else 
        echo "no samples to remove"
        cat ${Sample}.beagle > ${Sample}.subset.beagle
    fi

    pigz -f ${Sample}.subset.beagle -p ${SLURM_CPUS_PER_TASK}

    # Run ngsadmix
    mkdir ${input_name}
    /home/ap0y/angsd/angsd/misc/NGSadmix -likes ${Sample}.subset.beagle.gz -K 2 -o ${input_name}/${input_name} -P ${SLURM_CPUS_PER_TASK} -misTol 0.05 -minMaf 0 -minLrt 0 -minInd 0 -printInfo 1
        
    # Get likelihoods
    grep "like=" ${input_name}/${input_name}.log | cut -f2 -d " " | cut -f2 -d "=" >> ${input_name}/${input_name}.likes
    
    # Get a list of samples used for the ngsadmix run
    zcat ${Sample}.subset.beagle.gz | head -1 | cut --complement -f 1,2,3 | tr "\t" "\n" | uniq > ${input_name}/${input_name}_bams.txt
    
    # Zip any filter files to save space
    pigz -f -p ${SLURM_CPUS_PER_TASK} ${input_name}/*.filter
    
    # Partition ngsadmix results, making 2 files containing those > 0.7 in first or second column
    
    # add the sample names to qopts
    paste <(cat ${input_name}/${input_name}_bams.txt) <(cat ${input_name}/${input_name}.qopt) | tr ' ' '\t' > samples_to_partition
    
    # Define new partition filenames
    partition_prefix=${input_name}
    part1="${partition_prefix}_${partition_counter}a.txt"
    part2="${partition_prefix}_${partition_counter}b.txt"
    
    # Clear previous output files if they exist
    echo -n > "${part1}"
    echo -n > "${part2}"

    # Read the input file line by line - split samples by post prob > 0.5
    while IFS=$'\t' read -r sample_name col2_value col3_value; do
      # Ensure col2_value is numeric
      if [[ "$col2_value" =~ ^[0-9]+([.][0-9]+)?([eE][-+]?[0-9]+)?$ ]]; then
        # Check if the value in the second column is greater than 0.5
        if (( $(echo "$col2_value > 0.5" | bc -l) )); then
          echo "$sample_name" >> "${part1}"
        fi
      fi

      # Ensure col3_value is numeric
      if [[ "$col3_value" =~ ^[0-9]+([.][0-9]+)?([eE][-+]?[0-9]+)?$ ]]; then
        # Check if the value in the third column is greater than 0.5
        if (( $(echo "$col3_value > 0.5" | bc -l) )); then
          echo "$sample_name" >> "${part2}"
        fi
      fi
    done <samples_to_partition
    
    echo "Created $part1 and $part2"

    # Add new partitions to the list of files to process
    next_files+=("$part1" "$part2")
    
    #Copy outdir folder
    cp ${part1} ${input_name}/.
    cp ${part2} ${input_name}/.
    cp -r ${input_name} ${outdir}/${outname}/.
    
    # clean up to save space
    rm ${Sample}.subset.beagle.gz
    rm -rf ${input_name}
  done

  # Update the list of current files for the next iteration
  current_files=("${next_files[@]}")

  # Increment the partition counter
  partition_counter=$((partition_counter + 1))
done

# Remove temporary directory 
rm -rf ${tmp_dir}/*

# outdir useful job stats
/usr/local/bin/showJobStats.scr
